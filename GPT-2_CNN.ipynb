{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf10bf9-360b-40c2-b501-5c492dd99635",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio transformers datasets evaluate \\\n",
    "  rouge-score bert-score nltk --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb44c93-f910-46a6-8bff-0ced2d730a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5891a8-1817-4ca4-954e-6b1b35fb4152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using NVIDIA CUDA GPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import functools\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "import concurrent.futures  # (Keep only once)\n",
    "\n",
    "# Set device for GPU usage\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple MPS GPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"‚úÖ Using NVIDIA CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è Using CPU (no GPU available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b23c30-8df0-4ffe-9ccf-5ab397514781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "import pickle\n",
    "\n",
    "# ‚úÖ Set your NLTK data path explicitly\n",
    "nltk.data.path.append('')\n",
    "\n",
    "# ‚úÖ Load Punkt tokenizer manually\n",
    "with open('', 'rb') as f:\n",
    "    punkt_tokenizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fcb4bb7-c9d9-4b10-9765-9566ac6fcb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Preprocessing 100k training samples with checkpointing using 8 CPUs...\n",
      "üîÅ Loading existing checkpoint...\n",
      "‚úÖ Loaded 934112 samples from checkpoint!\n",
      "üîÑ Preprocessing remaining -834112 training samples using 8 CPUs...\n",
      "‚úÖ Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Sentence splitting\n",
    "def split_into_sentences(text):\n",
    "    return punkt_tokenizer.tokenize(text)\n",
    "\n",
    "# ‚úÖ Label sentences based on ROUGE-L\n",
    "def label_sentences(sentences, reference, top_k=3):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(reference, sent)['rougeL'].fmeasure for sent in sentences]\n",
    "    top_idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    return [1 if i in top_idxs else 0 for i in range(len(sentences))]\n",
    "\n",
    "# ‚úÖ Device and Tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Important: GPT-2 has no pad_token by default\n",
    "\n",
    "# ‚úÖ GPT-2 Extractive Summarizer Model\n",
    "class GPT2ExtractiveSummarizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = GPT2Model.from_pretrained(\"gpt2\")\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_rep = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_rep).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "# ‚úÖ Custom Dataset\n",
    "class ExtractiveDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        return {\n",
    "            \"input_ids\": sample[\"input_ids\"],\n",
    "            \"attention_mask\": sample[\"attention_mask\"],\n",
    "            \"label\": torch.tensor(sample[\"label\"], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# üì• Load CNN/DailyMail dataset\n",
    "raw_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "train_data = raw_dataset[\"train\"]\n",
    "val_data = raw_dataset[\"validation\"]\n",
    "test_data = raw_dataset[\"test\"]\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "# Only split sentences and label them in parallel\n",
    "def process_article(i):\n",
    "    if i >= len(train_articles):\n",
    "        return None\n",
    "    article = train_articles[i]['article']\n",
    "    summary = train_articles[i]['highlights']\n",
    "    sentences = split_into_sentences(article)\n",
    "    if not sentences:\n",
    "        return None\n",
    "    labels = label_sentences(sentences, summary)\n",
    "    return (sentences, labels)\n",
    "\n",
    "print(\"üîÑ Preprocessing 100k training samples with checkpointing using 8 CPUs...\")\n",
    "\n",
    "samples = []\n",
    "start_idx = 0\n",
    "sample_ckpt_path = \"\"\n",
    "\n",
    "# üîÑ Try to resume from checkpoint if exists\n",
    "if os.path.exists(sample_ckpt_path):\n",
    "    print(\"üîÅ Loading existing checkpoint...\")\n",
    "    samples = torch.load(sample_ckpt_path)\n",
    "    start_idx = len(samples)  # how many samples already saved\n",
    "    print(f\"‚úÖ Loaded {start_idx} samples from checkpoint!\")\n",
    "\n",
    "print(f\"üîÑ Preprocessing remaining {100000 - start_idx} training samples using 8 CPUs...\")\n",
    "\n",
    "train_articles = raw_dataset[\"train\"]\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    for idx, result in enumerate(executor.map(process_article, range(start_idx, 100000)), start=start_idx):\n",
    "        if result:\n",
    "            sentences, labels = result\n",
    "            tokenized = tokenizer(sentences, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "            for j in range(len(sentences)):\n",
    "                samples.append({\n",
    "                    \"input_ids\": tokenized['input_ids'][j],\n",
    "                    \"attention_mask\": tokenized['attention_mask'][j],\n",
    "                    \"label\": labels[j]\n",
    "                })\n",
    "        if (idx + 1) % 10000 == 0 or idx == 99999:\n",
    "            print(f\"‚úÖ Processed {idx+1} articles ‚Äî saving progress...\")\n",
    "            torch.save(samples, sample_ckpt_path)\n",
    "\n",
    "torch.save(samples, \"\")\n",
    "print(\"‚úÖ Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d086396f-e98f-4fd8-bd4b-e24b1d06eee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total preprocessed training samples: 934112\n"
     ]
    }
   ],
   "source": [
    "# üìä Check number of final training samples\n",
    "final_samples = torch.load(\"\")\n",
    "print(f\"‚úÖ Total preprocessed training samples: {len(final_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed583b33-ddb8-425c-b195-eede4c7a542e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using 4 GPUs (DataParallel)\n",
      "üîÅ Loading training checkpoint...\n",
      "‚úÖ Resuming from epoch 3\n",
      "\n",
      "üöÄ Training...\n"
     ]
    }
   ],
   "source": [
    "# üèãÔ∏è Training setup\n",
    "train_dataset = ExtractiveDataset(samples)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# ‚úÖ Move model to device first\n",
    "model = GPT2ExtractiveSummarizer().to(device)\n",
    "\n",
    "# ‚úÖ Forcefully use 4 GPUs if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"‚úÖ Using {torch.cuda.device_count()} GPUs (DataParallel)\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "save_path = \"best_gpt2_extractive.pt\"\n",
    "checkpoint_path = \"gpt2_extractive_checkpoint.pt\"\n",
    "\n",
    "start_epoch = 0\n",
    "best_rougel = 0.0\n",
    "num_epochs = 3 \n",
    "\n",
    "# üîÅ Resume training if checkpoint exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"üîÅ Loading training checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # If using DataParallel, wrap model temporarily\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    best_rougel = checkpoint['best_rougel']\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"‚úÖ Resuming from epoch {start_epoch}\")\n",
    "\n",
    "# üß™ Validation Evaluation\n",
    "def evaluate_rougel(model, val_data, tokenizer, device, max_samples=2000):\n",
    "    model.eval()\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    total_score = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(max_samples, len(val_data))):\n",
    "            article = val_data[i]['article']\n",
    "            reference = val_data[i]['highlights']\n",
    "            sentences = split_into_sentences(article)\n",
    "            if not sentences:\n",
    "                continue\n",
    "            tokenized = tokenizer(sentences, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "            input_ids = tokenized['input_ids'].to(device)\n",
    "            attention_mask = tokenized['attention_mask'].to(device)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            topk = torch.topk(logits, k=min(3, len(sentences))).indices.tolist()\n",
    "            pred_summary = \" \".join([sentences[i] for i in topk])\n",
    "            score = scorer.score(reference, pred_summary)['rougeL'].fmeasure\n",
    "            total_score += score\n",
    "    return total_score / max_samples\n",
    "\n",
    "# üèÉ Training Loop\n",
    "print(\"\\nüöÄ Training...\")\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    val_rougel = evaluate_rougel(model, val_data, tokenizer, device)\n",
    "    print(f\"üìà Validation ROUGE-L: {val_rougel:.4f}\")\n",
    "\n",
    "    if val_rougel > best_rougel:\n",
    "        best_rougel = val_rougel\n",
    "        # ‚úÖ Save .module.state_dict() because model is wrapped with DataParallel\n",
    "        torch.save(model.module.state_dict(), save_path)\n",
    "        print(f\"üíæ Best model saved (ROUGE-L {val_rougel:.4f})\")\n",
    "\n",
    "    # ‚úÖ Save entire model checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.module.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_rougel': best_rougel\n",
    "    }, checkpoint_path)\n",
    "    print(f\"üìå Checkpoint saved at epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e150a-c238-4891-8b0b-d3a395dfd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet', download_dir='')\n",
    "nltk.download('omw-1.4', download_dir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38d7efc1-9f02-4f8c-844e-4668b4aa451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"NLTK_DATA\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b2d21ab-4fc4-447e-8a39-c482d731f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Final Test Evaluation\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# ‚úÖ Correctly load for single GPU or multi-GPU\n",
    "if isinstance(model, nn.DataParallel):\n",
    "    model.module.load_state_dict(checkpoint)\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def evaluate_on_test(model, dataset, tokenizer, device, max_samples=10000):\n",
    "    model.eval()\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    references, predictions = [], []\n",
    "    meteor_total, r1_total, r2_total, rl_total = 0, 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(max_samples, len(dataset))):\n",
    "            article = dataset[i]['article']\n",
    "            reference = dataset[i]['highlights']\n",
    "            sentences = split_into_sentences(article)\n",
    "            if not sentences:\n",
    "                continue\n",
    "\n",
    "            tokenized = tokenizer(sentences, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "            input_ids = tokenized['input_ids'].to(device)    # [num_sentences, 128]\n",
    "            attention_mask = tokenized['attention_mask'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)   # [num_sentences]\n",
    "            topk = torch.topk(logits, k=min(3, len(sentences))).indices.tolist()\n",
    "\n",
    "            pred_summary = \" \".join([sentences[i] for i in topk])\n",
    "\n",
    "            scores = rouge.score(reference, pred_summary)\n",
    "            r1_total += scores['rouge1'].fmeasure\n",
    "            r2_total += scores['rouge2'].fmeasure\n",
    "            rl_total += scores['rougeL'].fmeasure\n",
    "            meteor_total += single_meteor_score(reference.split(), pred_summary.split())\n",
    "\n",
    "            references.append(reference)\n",
    "            predictions.append(pred_summary)\n",
    "\n",
    "    precision, recall, f1 = bert_score(predictions, references, lang='en', verbose=False)\n",
    "    n = len(predictions)\n",
    "    print(f\"\\nüìä Final Evaluation on {n} test samples\")\n",
    "    print(f\"ROUGE-1 F1: {r1_total / n:.4f}\")\n",
    "    print(f\"ROUGE-2 F1: {r2_total / n:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {rl_total / n:.4f}\")\n",
    "    print(f\"METEOR:     {meteor_total / n:.4f}\")\n",
    "    print(f\"BERTScore P/R/F1: {precision.mean().item():.4f} / {recall.mean().item():.4f} / {f1.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb76ca64-1b37-439e-9984-5197b7ed0843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Evaluation on 10000 test samples\n",
      "ROUGE-1 F1: 0.3249\n",
      "ROUGE-2 F1: 0.1158\n",
      "ROUGE-L F1: 0.1969\n",
      "METEOR:     0.2476\n",
      "BERTScore P/R/F1: 0.8535 / 0.8590 / 0.8562\n",
      "‚úÖ Summary pairs saved to gpt2_extractive_summary_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "evaluate_on_test(model, test_data, tokenizer, device)\n",
    "\n",
    "# ‚ú® Save 100 qualitative summaries\n",
    "summary_pairs = []\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        article = test_data[i]['article']\n",
    "        reference = test_data[i]['highlights']\n",
    "        sentences = split_into_sentences(article)\n",
    "        tokenized = tokenizer(sentences, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        input_ids = tokenized['input_ids'].to(device)\n",
    "        attention_mask = tokenized['attention_mask'].to(device)\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        topk = torch.topk(logits, k=min(3, len(sentences))).indices.tolist()\n",
    "        pred_summary = \" \".join([sentences[i] for i in topk])\n",
    "        summary_pairs.append({\n",
    "            \"article\": article[:500] + \"...\",\n",
    "            \"reference\": reference,\n",
    "            \"predicted_summary\": pred_summary\n",
    "        })\n",
    "\n",
    "pd.DataFrame(summary_pairs).to_csv(\"gpt2_extractive_summary_pairs.csv\", index=False)\n",
    "print(\"‚úÖ Summary pairs saved to gpt2_extractive_summary_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416368c9-c337-4bee-b806-0d5d33ddf34d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
