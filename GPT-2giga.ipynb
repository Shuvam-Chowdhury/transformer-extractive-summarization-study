{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf10bf9-360b-40c2-b501-5c492dd99635",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio transformers datasets evaluate \\\n",
    "  rouge-score bert-score nltk --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb44c93-f910-46a6-8bff-0ced2d730a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5891a8-1817-4ca4-954e-6b1b35fb4152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using NVIDIA CUDA GPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import functools\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "import concurrent.futures  # (Keep only once)\n",
    "\n",
    "# Set device for GPU usage\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple MPS GPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"‚úÖ Using NVIDIA CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è Using CPU (no GPU available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34b23c30-8df0-4ffe-9ccf-5ab397514781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "import pickle\n",
    "\n",
    "# ‚úÖ Set your NLTK data path explicitly\n",
    "nltk.data.path.append('')\n",
    "\n",
    "# ‚úÖ Load Punkt tokenizer manually\n",
    "with open('', 'rb') as f:\n",
    "    punkt_tokenizer = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f2c3e-1907-41fd-93ce-193eab8ec667",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceeb67dd-38cf-4b6b-8caf-44d4fd88cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb4bb7-c9d9-4b10-9765-9566ac6fcb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# üì• Load dataset\n",
    "raw_dataset = load_dataset(\"gigaword\")\n",
    "train_articles = raw_dataset[\"train\"]\n",
    "total_samples = 100000\n",
    "\n",
    "# üîß File paths\n",
    "save_dir = \"\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "final_path = os.path.join(save_dir, \"gpt2_final_samples.pt\")\n",
    "ckpt_path = os.path.join(save_dir, \"gpt2_samples_progress.pt\")\n",
    "\n",
    "# ‚úÖ Sentence splitting\n",
    "def split_into_sentences(text):\n",
    "    return punkt_tokenizer.tokenize(text)\n",
    "\n",
    "# ‚úÖ Label sentences with ROUGE-L\n",
    "def label_sentences(sentences, reference, top_k=3):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(reference, sent)['rougeL'].fmeasure for sent in sentences]\n",
    "    top_idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    return [1 if i in top_idxs else 0 for i in range(len(sentences))]\n",
    "\n",
    "# ‚úÖ Process one article\n",
    "def process_article(i):\n",
    "    if i >= len(train_articles): return None\n",
    "    article = train_articles[i]['document']\n",
    "    summary = train_articles[i]['summary']\n",
    "    sentences = split_into_sentences(article)\n",
    "    if not sentences: return None\n",
    "    labels = label_sentences(sentences, summary)\n",
    "    return sentences, labels\n",
    "\n",
    "# üîÅ Load checkpoint if exists\n",
    "samples = []\n",
    "start_idx = 0\n",
    "if os.path.exists(ckpt_path):\n",
    "    print(\"üîÅ Resuming from existing checkpoint...\")\n",
    "    samples = torch.load(ckpt_path)\n",
    "    start_idx = len(samples)\n",
    "    print(f\"‚úÖ Loaded {start_idx} preprocessed samples\")\n",
    "\n",
    "# üõ†Ô∏è Parallel preprocessing\n",
    "print(f\"üîÑ Preprocessing {total_samples - start_idx} remaining articles using 8 CPUs...\")\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    for idx, result in enumerate(executor.map(process_article, range(start_idx, total_samples)), start=start_idx):\n",
    "        if result:\n",
    "            sentences, labels = result\n",
    "            tokenized = tokenizer(sentences, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "            for j in range(len(sentences)):\n",
    "                samples.append({\n",
    "                    \"input_ids\": tokenized['input_ids'][j],\n",
    "                    \"attention_mask\": tokenized['attention_mask'][j],\n",
    "                    \"label\": labels[j]\n",
    "                })\n",
    "\n",
    "        # üíæ Checkpoint every 10k\n",
    "        if (idx + 1) % 10000 == 0 or (idx + 1) == total_samples:\n",
    "            print(f\"‚úÖ Processed {idx + 1} articles ‚Äî saving checkpoint...\")\n",
    "            torch.save(samples, ckpt_path)\n",
    "\n",
    "# üíæ Save final result\n",
    "torch.save(samples, final_path)\n",
    "print(\"‚úÖ Preprocessing complete and saved:\", final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d086396f-e98f-4fd8-bd4b-e24b1d06eee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total preprocessed training samples: 100313\n"
     ]
    }
   ],
   "source": [
    "# üìä Check number of final training samples\n",
    "final_samples = torch.load(\"\")\n",
    "print(f\"‚úÖ Total preprocessed training samples: {len(final_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed583b33-ddb8-425c-b195-eede4c7a542e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0434967ce5f24b8f84d77cd490443697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/94825 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d727926b1a31465895c366f2e473e6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/94826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Validation Samples: 94,825\n",
      "‚úÖ Test Samples      : 94,826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc34ec79eb14ff9902059c1be78f972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using 4 GPUs (DataParallel)\n",
      "\n",
      "üöÄ Training...\n",
      "‚úÖ Epoch 1 Avg Loss: 0.0209\n",
      "üìà Validation ROUGE-L: 0.2544\n",
      "üíæ Best model saved (ROUGE-L 0.2544)\n",
      "üìå Checkpoint saved at epoch 1\n",
      "‚úÖ Epoch 2 Avg Loss: 0.0002\n",
      "üìà Validation ROUGE-L: 0.2545\n",
      "üíæ Best model saved (ROUGE-L 0.2545)\n",
      "üìå Checkpoint saved at epoch 2\n",
      "‚úÖ Epoch 3 Avg Loss: 0.0001\n",
      "üìà Validation ROUGE-L: 0.2544\n",
      "üìå Checkpoint saved at epoch 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ‚úÖ Sentence splitting\n",
    "def split_into_sentences(text):\n",
    "    return punkt_tokenizer.tokenize(text)\n",
    "\n",
    "# ‚úÖ Label sentences based on ROUGE-L\n",
    "def label_sentences(sentences, reference, top_k=3):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(reference, sent)['rougeL'].fmeasure for sent in sentences]\n",
    "    top_idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    return [1 if i in top_idxs else 0 for i in range(len(sentences))]\n",
    "\n",
    "# ‚úÖ GPT-2 Extractive Summarizer Model\n",
    "class GPT2ExtractiveSummarizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = GPT2Model.from_pretrained(\"gpt2\")\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_rep = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_rep).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "# ‚úÖ Custom Dataset\n",
    "class ExtractiveDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        return {\n",
    "            \"input_ids\": sample[\"input_ids\"],\n",
    "            \"attention_mask\": sample[\"attention_mask\"],\n",
    "            \"label\": torch.tensor(sample[\"label\"], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# ‚úÖ Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ‚úÖ Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# üì• Load full validation split from Gigaword\n",
    "raw_dataset = load_dataset(\"gigaword\")\n",
    "val_data_full = raw_dataset[\"validation\"]\n",
    "\n",
    "# ‚úÖ Use Hugging Face's built-in split method\n",
    "split = val_data_full.train_test_split(test_size=0.5, seed=42)\n",
    "val_data = split[\"train\"]\n",
    "test_data = split[\"test\"]\n",
    "\n",
    "# üíæ Save to disk\n",
    "val_data.save_to_disk(\"\")\n",
    "test_data.save_to_disk(\"\")\n",
    "\n",
    "print(f\"‚úÖ Validation Samples: {len(val_data):,}\")\n",
    "print(f\"‚úÖ Test Samples      : {len(test_data):,}\")\n",
    "\n",
    "# üîÅ Load training samples\n",
    "samples = torch.load(\"\")\n",
    "\n",
    "# üèãÔ∏è Training setup\n",
    "train_dataset = ExtractiveDataset(samples)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = GPT2ExtractiveSummarizer().to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"‚úÖ Using {torch.cuda.device_count()} GPUs (DataParallel)\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "save_path = \"best_gpt2_extractive_gigaword.pt\"\n",
    "checkpoint_path = \"gpt2_extractive_checkpoint_gigaword.pt\"\n",
    "\n",
    "start_epoch = 0\n",
    "best_rougel = 0.0\n",
    "num_epochs = 3\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"üîÅ Loading training checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    best_rougel = checkpoint['best_rougel']\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"‚úÖ Resuming from epoch {start_epoch}\")\n",
    "\n",
    "# ‚úÖ ROUGE Evaluation Function\n",
    "def evaluate_rougel(model, val_data, tokenizer, device, max_samples=2000):\n",
    "    model.eval()\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    total_score = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(max_samples, len(val_data))):\n",
    "            article = val_data[i]['document']\n",
    "            reference = val_data[i]['summary']\n",
    "            sentences = split_into_sentences(article)\n",
    "            if not sentences:\n",
    "                continue\n",
    "            tokenized = tokenizer(sentences, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "            input_ids = tokenized['input_ids'].to(device)\n",
    "            attention_mask = tokenized['attention_mask'].to(device)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            topk = torch.topk(logits, k=min(3, len(sentences))).indices.tolist()\n",
    "            pred_summary = \" \".join([sentences[i] for i in topk])\n",
    "            score = scorer.score(reference, pred_summary)['rougeL'].fmeasure\n",
    "            total_score += score\n",
    "    return total_score / max_samples\n",
    "\n",
    "# üöÄ Training Loop\n",
    "print(\"\\nüöÄ Training...\")\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    val_rougel = evaluate_rougel(model, val_data, tokenizer, device)\n",
    "    print(f\"üìà Validation ROUGE-L: {val_rougel:.4f}\")\n",
    "\n",
    "    if val_rougel > best_rougel:\n",
    "        best_rougel = val_rougel\n",
    "        torch.save(model.module.state_dict(), save_path)\n",
    "        print(f\"üíæ Best model saved (ROUGE-L {val_rougel:.4f})\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.module.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_rougel': best_rougel\n",
    "    }, checkpoint_path)\n",
    "    print(f\"üìå Checkpoint saved at epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e150a-c238-4891-8b0b-d3a395dfd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet', download_dir='')\n",
    "nltk.download('omw-1.4', download_dir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38d7efc1-9f02-4f8c-844e-4668b4aa451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"NLTK_DATA\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b2d21ab-4fc4-447e-8a39-c482d731f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Final Test Evaluation\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# ‚úÖ Correctly load for single GPU or multi-GPU\n",
    "if isinstance(model, nn.DataParallel):\n",
    "    model.module.load_state_dict(checkpoint)\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def evaluate_on_test(model, dataset, tokenizer, device, max_samples=10000):\n",
    "    model.eval()\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    references, predictions = [], []\n",
    "    meteor_total, r1_total, r2_total, rl_total = 0, 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(max_samples, len(dataset))):\n",
    "            article = dataset[i]['document']       # ‚úÖ updated\n",
    "            reference = dataset[i]['summary'] \n",
    "            sentences = split_into_sentences(article)\n",
    "            if not sentences:\n",
    "                continue\n",
    "\n",
    "            tokenized = tokenizer(sentences, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "            input_ids = tokenized['input_ids'].to(device)    # [num_sentences, 128]\n",
    "            attention_mask = tokenized['attention_mask'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)   # [num_sentences]\n",
    "            topk = torch.topk(logits, k=min(3, len(sentences))).indices.tolist()\n",
    "\n",
    "            pred_summary = \" \".join([sentences[i] for i in topk])\n",
    "\n",
    "            scores = rouge.score(reference, pred_summary)\n",
    "            r1_total += scores['rouge1'].fmeasure\n",
    "            r2_total += scores['rouge2'].fmeasure\n",
    "            rl_total += scores['rougeL'].fmeasure\n",
    "            meteor_total += single_meteor_score(reference.split(), pred_summary.split())\n",
    "\n",
    "            references.append(reference)\n",
    "            predictions.append(pred_summary)\n",
    "\n",
    "    precision, recall, f1 = bert_score(predictions, references, lang='en', verbose=False)\n",
    "    n = len(predictions)\n",
    "    print(f\"\\nüìä Final Evaluation on {n} test samples\")\n",
    "    print(f\"ROUGE-1 F1: {r1_total / n:.4f}\")\n",
    "    print(f\"ROUGE-2 F1: {r2_total / n:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {rl_total / n:.4f}\")\n",
    "    print(f\"METEOR:     {meteor_total / n:.4f}\")\n",
    "    print(f\"BERTScore P/R/F1: {precision.mean().item():.4f} / {recall.mean().item():.4f} / {f1.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb76ca64-1b37-439e-9984-5197b7ed0843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8999479b78124589ab3495fc533989d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2fe83e87fd4ea78fbee2e237b14ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1eb4e6136342e0b2a0e0ffd042ccb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ce2eba203c467d86953f2e1c13bac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d657d6b5544b5c9b2fa748dfdace8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ac88572ad54ab9b22a95c8fe240033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Evaluation on 10000 test samples\n",
      "ROUGE-1 F1: 0.2904\n",
      "ROUGE-2 F1: 0.1029\n",
      "ROUGE-L F1: 0.2520\n",
      "METEOR:     0.3926\n",
      "BERTScore P/R/F1: 0.8348 / 0.9011 / 0.8664\n",
      "‚úÖ Summary pairs saved to gpt2_extractive_summary_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "evaluate_on_test(model, test_data, tokenizer, device)\n",
    "\n",
    "# ‚ú® Save 100 qualitative summaries\n",
    "summary_pairs = []\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        article = test_data[i]['document']      # ‚úÖ updated\n",
    "        reference = test_data[i]['summary']     # ‚úÖ updated\n",
    "        sentences = split_into_sentences(article)\n",
    "        tokenized = tokenizer(sentences, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        input_ids = tokenized['input_ids'].to(device)\n",
    "        attention_mask = tokenized['attention_mask'].to(device)\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        topk = torch.topk(logits, k=min(3, len(sentences))).indices.tolist()\n",
    "        pred_summary = \" \".join([sentences[i] for i in topk])\n",
    "        summary_pairs.append({\n",
    "            \"article\": article[:500] + \"...\",\n",
    "            \"reference\": reference,\n",
    "            \"predicted_summary\": pred_summary\n",
    "        })\n",
    "\n",
    "pd.DataFrame(summary_pairs).to_csv(\"gpt2_extractive_summary_pairs.csv\", index=False)\n",
    "print(\"‚úÖ Summary pairs saved to gpt2_extractive_summary_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416368c9-c337-4bee-b806-0d5d33ddf34d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4031d604-3314-4a44-a5fa-d2b2f34e1a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c561f-751e-4195-a60d-a522b102e263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219b007-f82a-4f31-af06-edc8e00afb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
